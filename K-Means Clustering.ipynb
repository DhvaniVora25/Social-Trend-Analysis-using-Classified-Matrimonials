{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Stores the advertisements from tribune india website to files according to the year it has crawled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from datetime import date, timedelta\n",
    "\n",
    "URL = 'https://www.tribuneindia.com/'\n",
    "\n",
    "def all_sundays(year):\n",
    "    # January 1st of the given year\n",
    "    dt = date(year, 1, 1)\n",
    "    # First Sunday of the given year\n",
    "    dt += timedelta(days=6 - dt.weekday())\n",
    "    while dt.year == year:\n",
    "        yield dt\n",
    "        dt += timedelta(days=7)\n",
    "\n",
    "def ads_from_page(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    ad_1 = soup.find_all('table')\n",
    "    brides_wanted = []\n",
    "    grooms_wanted = []\n",
    "\n",
    "    for ad in ad_1:\n",
    "        if \"Brides Wanted\" in str(ad):\n",
    "            ad_list = ad.find_all('p')\n",
    "            for i in range(len(ad_list)):\n",
    "                brides_wanted.append(' '.join(ad_list[i].get_text().replace('\\r', ' ').replace('\\n', ' ').rstrip().split()))\n",
    "                #' '.join(ad_2[i].get_text().replace('\\r', ' ').replace('\\n', ' ').rstrip().split())\n",
    "        if \"Grooms Wanted\" in str(ad):\n",
    "            ad_list = ad.find_all('p')\n",
    "            for i in range(len(ad_list)):\n",
    "                grooms_wanted.append(' '.join(ad_list[i].get_text().replace('\\r', ' ').replace('\\n', ' ').rstrip().split()))\n",
    "\n",
    "    f1.write(\"b'grooms wanted'\\n\")\n",
    "    for groom in brides_wanted:\n",
    "        groom = groom.encode('ascii', 'ignore')\n",
    "        f1.write(str(groom)+\"\\n\")\n",
    "\n",
    "    f1.write(\"b'brides wanted'\\n\")\n",
    "    for bride in grooms_wanted:\n",
    "        f1.write(str(bride.encode('ascii', 'ignore'))+\"\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   \n",
    "    for year in range(1998, 2000):\n",
    "        f1 = open(\"E:/Graduate stuff/Machine Learning and Deep Learning/matad/Advertisments by years/advertisments\"+str(year)+\".txt\", \"w\")\n",
    "\n",
    "        for sunday in all_sundays(year):\n",
    "            d = sunday.strftime('%y%b%d')\n",
    "            if year == 2018 and d == '18dec16':\n",
    "                break\n",
    "            url = URL+str(year)+'/'+str(d.lower())+'/class.htm'\n",
    "            print(url)\n",
    "            ads_from_page(url)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It stores the advertisements in two seperate lists of brides and grooms according to the years. For example, brides[i] is the list of all advertisements by the brides in the year 1998 + i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grooms_brides(filename):\n",
    "    grooms = []\n",
    "    brides = []\n",
    "    groom = 0\n",
    "    with open(filename,\"r\") as f:\n",
    "        for line in f:\n",
    "            if \"brides\" in line.lower():\n",
    "                groom = 0\n",
    "                continue\n",
    "            elif \"grooms\" in line.lower():\n",
    "                groom = 1\n",
    "                continue\n",
    "            elif groom == 0:\n",
    "                grooms.append(line[2:-2])\n",
    "            else:\n",
    "                brides.append(line[2:-2])\n",
    "\n",
    "    return grooms, brides\n",
    "\n",
    "brides = []\n",
    "grooms = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    for year in range(1998, 2000):\n",
    "        groomsy, bridesy = grooms_brides(\"E:/Graduate stuff/Machine Learning and Deep Learning/matad/Advertisments by years/advertisments\"+str(year)+\".txt\")\n",
    "        brides.append(bridesy)\n",
    "        grooms.append(groomsy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ad2013_brides = \"E:/Graduate stuff/Machine Learning and Deep Learning/matad/Corpus/Brides (Grooms wanted)/Brides_tribuneindia_2013.txt\"\n",
    "brides2013 = []\n",
    "\n",
    "f = open(ad2013_brides, encoding = \"utf8\")\n",
    "line = \"\\n\"\n",
    "\n",
    "while line:\n",
    "    line = f.readline()\n",
    "    if str(line)!= \"b'\\\\n'\":\n",
    "        brides2013.append(line)   \n",
    "f.close()\n",
    "\n",
    "brides.append(brides2013)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I am going to use document clustering to understand the trends in this.\n",
    "\n",
    "http://brandonrose.org/clustering <reference>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "context = [\"suitab\",\"match\",\"prefer\",\"girl\",\"boy\",\"man\",\"from\",\"suitabl\",\"suitable\",\"preferr\",\"preferred\",\"gmail\"]\n",
    "context1 = ['born', 'contact', 'email', 'famili', 'girl', 'gmail.com', 'match', 'prefer', 'profession', 'profession qualifi', 'qualifi', 'settl','suitabl', 'suitabl match', 'work', 'yahoo.com']\n",
    "stopwords.extend(context)\n",
    "stopwords.extend(context1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "One Stemmed and the other Tokenized Vocab from all the advertisements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for year in range(1998,2001):\n",
    "    for bride in brides[year-1998]:\n",
    "        allwords_stemmed = tokenize_and_stem(bride) #for each item in 'synopses', tokenize/stem\n",
    "        totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "        allwords_tokenized = tokenize_only(bride)\n",
    "        totalvocab_tokenized.extend(allwords_tokenized)\n",
    "    if year == 2000:\n",
    "        break\n",
    "    for groom in grooms[year-1998]:\n",
    "        allwords_stemmed = tokenize_and_stem(groom) #for each item in 'synopses', tokenize/stem\n",
    "        totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "        allwords_tokenized = tokenize_only(groom)\n",
    "        totalvocab_tokenized.extend(allwords_tokenized)\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NOT PROPER RIGHT NOW - NEED TO CHANGE\n",
    "\n",
    "Using these two lists, I create a pandas DataFrame with the stemmed vocabulary as the index and the tokenized words as the column. The benefit of this is it provides an efficient way to look up a stem and return a full token. The downside here is that stems to tokens are one to many: the stem 'run' could be associated with 'ran', 'runs', 'running', etc. For my purposes this is fine--I'm perfectly happy returning the first token associated with the stem I need to look up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print(\"total vocab tokenized: \")\n",
    "print(len(totalvocab_tokenized))\n",
    "print(\"total vocab stemmed: \")\n",
    "print(len(totalvocab_stemmed))\n",
    "print(\"total vocab: \")\n",
    "#print(vocab_frame)\n",
    "#print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab_frame.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TF IDF matrix is used for k-means clustering. So there will be mainly two differnet plots - one for the brides and other for the grooms.\n",
    "\n",
    "The following code is just for a year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This is taking into account the English stopwords..... That will cause another issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.93, max_features=200000,\n",
    "                                 min_df=0.08, stop_words=stopwords,\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix_brides = tfidf_vectorizer.fit_transform(brides[0]) #fit the vectorizer to brides\n",
    "#%time tfidf_matrix_grooms = tfidf_vectorizer.fit_transform(grooms[0]) #fit the vectorizer to grooms\n",
    "%time tfidf_matrix_brides2013 = tfidf_vectorizer.fit_transform(brides[2]) #fit the vectorizer to brides 2013\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist_brides = 1 - cosine_similarity(tfidf_matrix_brides2013)\n",
    "dist_brides1999 = 1 - cosine_similarity(tfidf_matrix_brides)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's do the clustering for the time being :P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix_brides2013)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km1 = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km1.fit(tfidf_matrix_brides)\n",
    "\n",
    "clusters1 = km1.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films = {'ads':brides[2], 'cluster':clusters}\n",
    "frame = pd.DataFrame(films, index = [clusters])\n",
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films1 = {'ads':brides[0], 'cluster':clusters1}\n",
    "frame = pd.DataFrame(films1, index = [clusters1])\n",
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :10]: #replace 6 with n words per cluster\n",
    "        #print(terms[ind])\n",
    "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "\"\"\"\n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in frame.iloc[i]['ad'].values.tolist():\n",
    "        print(' %s,' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "\"\"\"    \n",
    "print()\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
